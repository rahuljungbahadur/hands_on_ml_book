{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPklkALTIydJlYHzLV/2zA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahuljungbahadur/hands_on_ml_book/blob/main/fashionMNIST_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning on Fashion MNIST\n",
        "\n",
        "A transfer learning example for Fashion-MNISt dataset. \n",
        "\n",
        "Subset the dataset so that the `main` set consists of 8 classes and the `transferLearning` one consists of 2. Then apply the learnt weights and biases to the 2 classes subset\n"
      ],
      "metadata": {
        "id": "vbuZlJ3uFDje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "JQ6NxhfsJLph"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and subset it"
      ],
      "metadata": {
        "id": "ClRwQygOJc6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(fashion_mnist.load_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Vri0__J4_K",
        "outputId": "26d28ade-a8a1-4dfa-bcd2-3454bf0d8748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function load_data in module keras.datasets.fashion_mnist:\n",
            "\n",
            "load_data()\n",
            "    Loads the Fashion-MNIST dataset.\n",
            "    \n",
            "    This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n",
            "    along with a test set of 10,000 images. This dataset can be used as\n",
            "    a drop-in replacement for MNIST.\n",
            "    \n",
            "    The classes are:\n",
            "    \n",
            "    | Label | Description |\n",
            "    |:-----:|-------------|\n",
            "    |   0   | T-shirt/top |\n",
            "    |   1   | Trouser     |\n",
            "    |   2   | Pullover    |\n",
            "    |   3   | Dress       |\n",
            "    |   4   | Coat        |\n",
            "    |   5   | Sandal      |\n",
            "    |   6   | Shirt       |\n",
            "    |   7   | Sneaker     |\n",
            "    |   8   | Bag         |\n",
            "    |   9   | Ankle boot  |\n",
            "    \n",
            "    Returns:\n",
            "      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
            "    \n",
            "    **x_train**: uint8 NumPy array of grayscale image data with shapes\n",
            "      `(60000, 28, 28)`, containing the training data.\n",
            "    \n",
            "    **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n",
            "      with shape `(60000,)` for the training data.\n",
            "    \n",
            "    **x_test**: uint8 NumPy array of grayscale image data with shapes\n",
            "      (10000, 28, 28), containing the test data.\n",
            "    \n",
            "    **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n",
            "      with shape `(10000,)` for the test data.\n",
            "    \n",
            "    Example:\n",
            "    \n",
            "    ```python\n",
            "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
            "    assert x_train.shape == (60000, 28, 28)\n",
            "    assert x_test.shape == (10000, 28, 28)\n",
            "    assert y_train.shape == (60000,)\n",
            "    assert y_test.shape == (10000,)\n",
            "    ```\n",
            "    \n",
            "    License:\n",
            "      The copyright for Fashion-MNIST is held by Zalando SE.\n",
            "      Fashion-MNIST is licensed under the [MIT license](\n",
            "      https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT9ktpl8JQsL",
        "outputId": "24969158-c1cf-4bb7-b343-4c37ef5df87e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "k51HeFifJksU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[0])   ## its a shoe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "FGcU4LavJtc9",
        "outputId": "6a2b15db-bc34-48ba-d921-185b990434e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcc03da2750>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's subset `Shirt` and `Sneaker`. THese are the items encoded as `6` and `7` respectively."
      ],
      "metadata": {
        "id": "-1AxRZjJKCWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subsetting train and test data into main and tl subsets\n",
        "\n",
        "Train the model on the `main` subset and then used the learnt weights and biases on the `tl` subset"
      ],
      "metadata": {
        "id": "KSMzc_gONqJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_indices = (y_train!=6)&(y_train!=7)"
      ],
      "metadata": {
        "id": "7qM1k9VzJw7I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_main = X_train[main_indices]"
      ],
      "metadata": {
        "id": "hL8N-dxMKcBg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu1oXodYLan2",
        "outputId": "83c864c5-6b8b-4868-b159-35af739afc55"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_main.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzmKxEDLX5W",
        "outputId": "3528616d-e005-4f00-998c-3fbf23bca1d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_main = y_train[main_indices]"
      ],
      "metadata": {
        "id": "PSqmRwERLZUh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_test_indices = np.logical_not(np.isin(y_test, [6,7]))"
      ],
      "metadata": {
        "id": "UdwGp_kwLxn2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_main = X_test[main_test_indices]\n",
        "y_test_main = y_test[main_test_indices]"
      ],
      "metadata": {
        "id": "DsKsx2vfLjvh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##TL indices\n",
        "tl_train_indices = np.isin(y_train, [6,7])\n",
        "X_train_tl = X_train[tl_train_indices]\n",
        "y_train_tl = y_train[tl_train_indices]"
      ],
      "metadata": {
        "id": "PcYIwCD5Lspz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tl.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqw_LzzEMxj4",
        "outputId": "0a287069-f894-4721-cda7-d96a23858b77"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample of X_tl\n",
        "plt.imshow(X_train_tl[6])  ## Has Shirt\n",
        "plt.imshow(X_train_tl[0])  ## Has Sneakers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "dAo4rpWdM1-j",
        "outputId": "e4346cb3-7c4d-45aa-a2e0-5a270703baf5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcc038c2a50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQQUlEQVR4nO3df2xd9XnH8c/j37GTQEIgzUKUAGVjaRmBudANOlFQGaSqoNuEyB9dJmUK0kBqpaobYtKKNE1CHbTrtIkRStS060BIFBGksJJlVIj9yDBRRgKUJkBCYpwYCCYmaRxf+9kfPnQm+DzHuef+ot/3S7J8fZ577nl8k4/vj+/9nq+5uwD86mtrdgMAGoOwA4kg7EAiCDuQCMIOJKKjkQfrsm7vUV8jDwkk5YSO6aSP2Uy1UmE3s+slfVdSu6Tvufvd0fV71Kcr7NoyhwQQ2O7bcmtVP403s3ZJ/yjpBkkrJa0xs5XV3h6A+irzmv1ySXvd/TV3PynpYUk31qYtALVWJuxLJR2Y9vPBbNuHmNl6Mxsws4FxjZU4HIAy6v5uvLtvcPd+d+/vVHe9DwcgR5mwD0paNu3nc7NtAFpQmbA/J+lCMzvPzLok3SJpc23aAlBrVQ+9uXvFzG6X9BNNDb1tdPcXa9YZgJoqNc7u7lskbalRLwDqiI/LAokg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiSi3ZbGb7JI1KmpBUcff+WjQFoPZKhT3zeXd/uwa3A6COeBoPJKJs2F3SU2b2vJmtn+kKZrbezAbMbGBcYyUPB6BaZZ/GX+Xug2Z2jqStZvYzd39m+hXcfYOkDZI03xZ6yeMBqFKpR3Z3H8y+D0t6TNLltWgKQO1VHXYz6zOzeR9clnSdpN21agxAbZV5Gr9Y0mNm9sHt/Iu7/2tNugJQc1WH3d1fk3RJDXsBUEcMvQGJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJqMUJJ1HW1DTh6nn+CYCsI/4n9omJqm9bkqyzK959/GR8+2W0tcf1yYLfrY6suzus+8ngfim4z6vFIzuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4lgnL0V1GlcVZJkBX/PvVLq5us5jn7wzt8N63+/7v6w/q0LLq5lO6fFx1pvqTMe2YFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSATj7B8HRfPdg3H6us4nlzR8WzwWPnJx/jj+Pdc8HO57qPJOWB84fn5Yf/uJX8+tLfrSz8N9y2rr6Qnre/760tzaBd/4r1q3I2kWj+xmttHMhs1s97RtC81sq5ntyb4vqEt3AGpmNk/jvy/p+lO23SFpm7tfKGlb9jOAFlYYdnd/RtKRUzbfKGlTdnmTpJtq3BeAGqv2Nftidx/KLh+StDjvima2XtJ6SepRb5WHA1BW6Xfj3d0l5b5D5O4b3L3f3fs7FZ+ED0D9VBv2w2a2RJKy78O1awlAPVQb9s2S1maX10p6vDbtAKiXwtfsZvaQpKslLTKzg5K+KeluSY+Y2TpJ+yXdXM8mP/ZKjJPPqh4d+tJPhfVXb5kf1s/vPxDWf/ob94b1fz6aP9b91Ejc24Fj8YjuDee8GNYf+a2NubU/01XhvmW9eetlYf2Cy96o6/FnUhh2d1+TU7q2xr0AqCM+LgskgrADiSDsQCIIO5AIwg4k4ldnimvJ5XuLpiROnjhxuh39v5Knim5ffE5Yf+Wepbm1R6/6p3DfwYkzwvpPj/5mWP/zN68J63Pb80+pfHbX++G+T792YVg/viheLnr1D7+RW1uheBppx/JlYf31P47rA7f+XVj/wy+uza2dvOa3w307/v35sJ6HR3YgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJhXs/lgk8x3xb6FVZislwwVdTa43F2r5RbmriMY390RVgfuik+3fOTn/uHsL7jxLm5tW0jK8N9fzHRGdZX9Manc17SNRLWh8fzp9AeGoun1y7reTes7xiJx7ovOWMwt/b783aF+x4q+PzBfW9cHdb1haGw3D63L7dmC+JjV/bnTzve7tt01I/MGBQe2YFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSMTHa5y9id74q/yliW+/5Ylw38/17gnrT45eHNb3nzgrrEdj5VfMfy3ct8i4lzvlwXsTc3Jrrx9fFO87Hp9j4NzeeIx/Qcfx3NrTh/NPcS1J3dftC+tFXv3b3wnr3/uD+3NrT4ysCvd9ad1FubX//tkDeu/4m4yzAykj7EAiCDuQCMIOJIKwA4kg7EAiCDuQiJYaZ5/4fLzM7RvXdefW2j8Zn4N8Tnc8Z/ySc94M60t78sd0j1bi8eBDJ+J52+f1xXPG2zUZ1hd15v/u3W3j4b5DJ88M6/Pa4/Plt1nc23uV3tzayHh+TZKOVvL/vSVp0uPHqqi3tef8R7hvu+JcLO84Gta3HIvPt7/j6PLc2uLu+LYf2v7Z3Nqhv/muxvYfrG6c3cw2mtmwme2etu0uMxs0s53Z1+qi2wHQXLN5Gv99SdfPsP077r4q+9pS27YA1Fph2N39GUlHGtALgDoq8wbd7Wb2QvY0f0HelcxsvZkNmNnAuPLX/QJQX9WG/T5JF0haJWlI0r15V3T3De7e7+79nYrfcAFQP1WF3d0Pu/uEu09KekDS5bVtC0CtVRV2M1sy7ccvS9qdd10AraFwsrKZPSTpakmLzOygpG9KutrMVklySfsk3Tqbg51c0qcDf5o/L/yy1S+F+3+6O388uWgs+mglf161JPV1xO8nHA7OcV401vxrc94L65XJ+G/ugRO5b4lIkvb62bm1nvZ4nL0yGZ9vf2FX/pxwqfh3X9CZv3/RZwDO7orrZ3UeC+vRZwT2jH0i3PeEx+fT31UwDn98Ml47flHwf3lFz9vhvtUqDLu7r5lh84N16AVAHfFxWSARhB1IBGEHEkHYgUQQdiAR5c4TfJq63xrTivvzT6s8+Nwnw/0HrgyGQy6Kp7iuWpq/fK8kLZ8TTzNd2Zs/BbavLR62OzEZD+N0Wryc9GfmToT1K3ryl/AdV/4y15LUY/EQ0hlt8dBcr8VDTJ0W7x95oxL/mx4Ips9K0shkfv3YZLnps29V4mnLZ7THQ5aDY/lTi9+t5C/nLEnLnsyvvRPMjuWRHUgEYQcSQdiBRBB2IBGEHUgEYQcSQdiBRDR0nF2S1JY/7jtne7y08fKfxFNFI+/1xmOyz37qM2H93Yvm5tZGl8dj2SeWxOPk3h3XC4bKpbZgrHwy3rnjnfgzAB3H4v27C85O2D2S31vPSPx7dx+JT//d/n78+Ya20V+E9Yj3xJ8fkBX9oxR4czi39MpI/LmLOf4/ubU2zx/f55EdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFENHSc3SsVTRzOH19sP/OMcP+O81fk33Ywfj8bbcP5SzJL0ll7D+bWFvXFY/g+Fo8XF7GOgjnh0bLb7fG+3hsvN62CY3t3PE4/2ZW//0RvvO/J+XG98ol4TvrJeflzxgtOMaCCs1xrsiA5ld74/2Pn6MLcWvt4fI6B+a8Hy2jv/M/cEo/sQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kovHz2QMTIwXz1YvqJbTNmxfWrTuY31yJ5x/rzPi2fU48d3qyq/p/Ju+I/54XfT7BKvGSzIXHb88/vkWfD5DUNRJ/PqF3X3xu9mjOuXcWfH6g6D4vul8K7vdo/7bR+Pea2Pt6ftHz5/AXPrKb2TIze9rMXjKzF83sq9n2hWa21cz2ZN/jRcQBNNVsnsZXJH3d3VdK+qyk28xspaQ7JG1z9wslbct+BtCiCsPu7kPuviO7PCrpZUlLJd0oaVN2tU2SbqpXkwDKO60Xg2a2QtKlkrZLWuzuQ1npkKTFOfusl7ReknoUf4YcQP3M+t14M5sr6VFJX3P3Dy0f5+4uacZ3W9x9g7v3u3t/p+KJCwDqZ1ZhN7NOTQX9R+7+42zzYTNbktWXSMqfzgag6QqfxpuZSXpQ0svu/u1ppc2S1kq6O/v+eF06bJDJ0dH4CgXl0KES+9ZZyRMil7r9sscuOAH3x1a9fq/ZvGa/UtJXJO0ys53Ztjs1FfJHzGydpP2Sbq5PiwBqoTDs7v6s8v8IX1vbdgDUCx+XBRJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJRGHYzW2ZmT5vZS2b2opl9Ndt+l5kNmtnO7Gt1/dsFUK3ZrM9ekfR1d99hZvMkPW9mW7Pad9z9nvq1B6BWZrM++5CkoezyqJm9LGlpvRsDUFun9ZrdzFZIulTS9mzT7Wb2gpltNLMFOfusN7MBMxsY11ipZgFUb9ZhN7O5kh6V9DV3PyrpPkkXSFqlqUf+e2faz903uHu/u/d3qrsGLQOoxqzCbmadmgr6j9z9x5Lk7ofdfcLdJyU9IOny+rUJoKzZvBtvkh6U9LK7f3va9iXTrvZlSbtr3x6AWpnNu/FXSvqKpF1mtjPbdqekNWa2SpJL2ifp1rp0CKAmZvNu/LOSbIbSltq3A6Be+AQdkAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiTC3L1xBzN7S9L+aZsWSXq7YQ2cnlbtrVX7kuitWrXsbbm7nz1ToaFh/8jBzQbcvb9pDQRatbdW7Uuit2o1qjeexgOJIOxAIpod9g1NPn6kVXtr1b4keqtWQ3pr6mt2AI3T7Ed2AA1C2IFENCXsZna9mb1iZnvN7I5m9JDHzPaZ2a5sGeqBJvey0cyGzWz3tG0LzWyrme3Jvs+4xl6TemuJZbyDZcabet81e/nzhr9mN7N2ST+X9AVJByU9J2mNu7/U0EZymNk+Sf3u3vQPYJjZ70l6X9IP3P3T2bZvSTri7ndnfygXuPtftEhvd0l6v9nLeGerFS2Zvsy4pJsk/YmaeN8Ffd2sBtxvzXhkv1zSXnd/zd1PSnpY0o1N6KPlufszko6csvlGSZuyy5s09Z+l4XJ6awnuPuTuO7LLo5I+WGa8qfdd0FdDNCPsSyUdmPbzQbXWeu8u6Skze97M1je7mRksdveh7PIhSYub2cwMCpfxbqRTlhlvmfuumuXPy+INuo+6yt0vk3SDpNuyp6styadeg7XS2OmslvFulBmWGf+lZt531S5/XlYzwj4oadm0n8/NtrUEdx/Mvg9LekyttxT14Q9W0M2+Dze5n19qpWW8Z1pmXC1w3zVz+fNmhP05SRea2Xlm1iXpFkmbm9DHR5hZX/bGicysT9J1ar2lqDdLWptdXivp8Sb28iGtsox33jLjavJ91/Tlz9294V+SVmvqHflXJf1lM3rI6et8Sf+bfb3Y7N4kPaSpp3XjmnpvY52ksyRtk7RH0r9JWthCvf1Q0i5JL2gqWEua1NtVmnqK/oKkndnX6mbfd0FfDbnf+LgskAjeoAMSQdiBRBB2IBGEHUgEYQcSQdiBRBB2IBH/B6T+4ESSsYsYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TL test indices\n",
        "tl_test_indices = np.isin(y_test, [6,7])\n",
        "X_test_tl = X_test[tl_test_indices]\n",
        "y_test_tl = y_test[tl_test_indices]"
      ],
      "metadata": {
        "id": "bUlORIaUMubw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating main model"
      ],
      "metadata": {
        "id": "k1MUYbdwNfyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Input\n",
        "from tensorflow.keras.activations import elu, relu, softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint"
      ],
      "metadata": {
        "id": "0fsfryTsMwl8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_main.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0lkuHIOeq2",
        "outputId": "37c495b2-02e5-4929-99b4-2668a34e17c4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(Model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCpmGrTRS-c",
        "outputId": "c394cdb6-533f-4717-c007-29306d9c7233"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Model in module keras.engine.training:\n",
            "\n",
            "class Model(keras.engine.base_layer.Layer, keras.utils.version_utils.ModelVersionSelector)\n",
            " |  Model(*args, **kwargs)\n",
            " |  \n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Args:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n",
            " |  inputs are not supported (e.g. lists of list or dicts of dict).\n",
            " |  \n",
            " |  A new Functional API model can also be created by using the\n",
            " |  intermediate tensors. This enables you to quickly extract sub-components\n",
            " |  of the model.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  inputs = keras.Input(shape=(None, None, 3))\n",
            " |  processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n",
            " |  conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n",
            " |  pooling = keras.layers.GlobalAveragePooling2D()(conv)\n",
            " |  feature = keras.layers.Dense(10)(pooling)\n",
            " |  \n",
            " |  full_model = keras.Model(inputs, feature)\n",
            " |  backbone = keras.Model(processed, conv)\n",
            " |  activations = keras.Model(conv, feature)\n",
            " |  ```\n",
            " |  \n",
            " |  Note that the `backbone` and `activations` models are not\n",
            " |  created with `keras.Input` objects, but with the tensors that are originated\n",
            " |  from `keras.Inputs` objects. Under the hood, the layers and weights will\n",
            " |  be shared across these models, so that user can train the `full_model`, and\n",
            " |  use `backbone` or `activations` to do feature extraction.\n",
            " |  The inputs and outputs of the model can be nested structures of tensors as\n",
            " |  well, and the created models are standard Functional API models that support\n",
            " |  all the existing APIs.\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__()` and you should implement the model's forward pass\n",
            " |  in `call()`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super().__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call()`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super().__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      keras.utils.version_utils.LayerVersionSelector\n",
            " |      keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __copy__(self)\n",
            " |  \n",
            " |  __deepcopy__(self, memo)\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __reduce__(self)\n",
            " |      Helper for pickle.\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes,\n",
            " |         where shapes are tuples, integers, or `TensorShape` instances.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, `TensorShape`, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or keyword arg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs and returns the outputs as tensors.\n",
            " |      \n",
            " |      In this case `call()` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Note: This method should not be called directly. It is only meant to be\n",
            " |      overridden when subclassing `tf.keras.Model`.\n",
            " |      To call a model on an input, always use the `__call__()` method,\n",
            " |      i.e. `model(inputs)`, which relies on the underlying `call()` method.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs: Input tensor, or dict/list/tuple of input tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be either a boolean tensor or\n",
            " |            None (no mask). For more details, check the guide\n",
            " |              [here](https://www.tensorflow.org/guide/keras/masking_and_padding).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
            " |                    loss=tf.keras.losses.BinaryCrossentropy(),\n",
            " |                    metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
            " |                             tf.keras.metrics.FalseNegatives()])\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: Loss function. Maybe be a string (name of loss function), or\n",
            " |            a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where `y_true` are the ground truth values, and\n",
            " |            `y_pred` are the model's predictions.\n",
            " |            `y_true` should have shape\n",
            " |            `(batch_size, d0, .. dN)` (except in the case of\n",
            " |            sparse loss functions such as\n",
            " |            sparse categorical crossentropy which expects integer arrays of shape\n",
            " |            `(batch_size, d0, .. dN-1)`).\n",
            " |            `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
            " |            The loss function should return a float tensor.\n",
            " |            If a custom `Loss` instance is\n",
            " |            used and reduction is set to `None`, return value has shape\n",
            " |            `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
            " |            values; otherwise, it is a scalar. If the model has multiple outputs,\n",
            " |            you can use a different loss on each output by passing a dictionary\n",
            " |            or a list of losses. The loss value that will be minimized by the\n",
            " |            model will then be the sum of all individual losses, unless\n",
            " |            `loss_weights` is specified.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |            `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |            You can also pass a list to specify a metric or a list of metrics\n",
            " |            for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
            " |            or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |            strings 'accuracy' or 'acc', we convert this to one of\n",
            " |            `tf.keras.metrics.BinaryAccuracy`,\n",
            " |            `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |            `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |            function used and the model output shape. We do a similar\n",
            " |            conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            `sample_weight` or `class_weight` during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`. `run_eagerly=True` is not supported when using\n",
            " |            `tf.distribute.experimental.ParameterServerStrategy`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False, **kwargs)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |          **kwargs: Unused at this time.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      `Model.evaluate` is not yet supported with\n",
            " |      `tf.distribute.experimental.ParameterServerStrategy`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
            " |              callable that takes a single argument of type\n",
            " |              `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
            " |              `DatasetCreator` should be used when users prefer to specify the\n",
            " |              per-replica batching and sharding logic for the `Dataset`.\n",
            " |              See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
            " |              information.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below. If using\n",
            " |            `tf.distribute.experimental.ParameterServerStrategy`, only\n",
            " |            `DatasetCreator` type is supported for `x`.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided\n",
            " |              (unless the `steps_per_epoch` flag is set to\n",
            " |              something other than None).\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              'auto' defaults to 1 for most cases, but 2 when used with\n",
            " |              `ParameterServerStrategy`. Note that the progress bar is not\n",
            " |              particularly useful when logged to a file, so verbose=2 is\n",
            " |              recommended when not running interactively (eg, in a production\n",
            " |              environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |              Callbacks with batch-level calls are currently unsupported with\n",
            " |              `tf.distribute.experimental.ParameterServerStrategy`, and users are\n",
            " |              advised to implement epoch-level calls instead with an appropriate\n",
            " |              `steps_per_epoch` value.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |              `validation_split` is not yet supported with\n",
            " |              `tf.distribute.experimental.ParameterServerStrategy`.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
            " |                - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n",
            " |                - A `tf.data.Dataset`.\n",
            " |                - A Python generator or `keras.utils.Sequence` returning\n",
            " |                `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
            " |              `validation_data` is not yet supported with\n",
            " |              `tf.distribute.experimental.ParameterServerStrategy`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator or an object of tf.data.Dataset.\n",
            " |              'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n",
            " |              will run indefinitely with an infinitely repeating dataset.\n",
            " |              This argument is not supported with array inputs.\n",
            " |              When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
            " |                * `steps_per_epoch=None` is not supported.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
            " |      every time it is called. The callers should make a copy of the returned dict\n",
            " |      if they want to modify it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Args:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Args:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`). This can also be a path to a SavedModel\n",
            " |              saved from `model.save`.\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If `h5py` is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self, force=False)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called. You can skip the cache and generate again the\n",
            " |      function with `force=True`.\n",
            " |      \n",
            " |      Args:\n",
            " |        force: Whether to regenerate the predict function and skip the cached\n",
            " |          function if available.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self, force=False)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called. You can skip the cache and generate again the\n",
            " |      function with `force=True`.\n",
            " |      \n",
            " |      Args:\n",
            " |        force: Whether to regenerate the test function and skip the cached\n",
            " |          function if available.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self, force=False)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called. You can skip the cache and generate again the\n",
            " |      function with `force=True`.\n",
            " |      \n",
            " |      Args:\n",
            " |        force: Whether to regenerate the train function and skip the cached\n",
            " |          function if available.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__()` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict()` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in a `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
            " |                model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
            " |                multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Args:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Args:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_spec(self, dynamic_batch=True)\n",
            " |      Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`.\n",
            " |      \n",
            " |      This value is automatically defined after calling the model for the first\n",
            " |      time. Afterwards, you can use it when exporting the model for serving:\n",
            " |      \n",
            " |      ```python\n",
            " |      model = tf.keras.Model(...)\n",
            " |      \n",
            " |      @tf.function\n",
            " |      def serve(*args, **kwargs):\n",
            " |        outputs = model(*args, **kwargs)\n",
            " |        # Apply postprocessing steps, or add additional outputs.\n",
            " |        ...\n",
            " |        return outputs\n",
            " |      \n",
            " |      # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is\n",
            " |      # an empty dict since functional models do not use keyword arguments.\n",
            " |      arg_specs, kwarg_specs = model.save_spec()\n",
            " |      \n",
            " |      model.save(path, signatures={\n",
            " |        'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)\n",
            " |      })\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        dynamic_batch: Whether to set the batch sizes of all the returned\n",
            " |          `tf.TensorSpec` to `None`. (Note that when defining functional or\n",
            " |          Sequential models with `tf.keras.Input([...], batch_size=X)`, the\n",
            " |          batch size will always be preserved). Defaults to `True`.\n",
            " |      Returns:\n",
            " |        If the model inputs are defined, returns a tuple `(args, kwargs)`. All\n",
            " |        elements in `args` and `kwargs` are `tf.TensorSpec`.\n",
            " |        If the model inputs are not defined, returns `None`.\n",
            " |        The model inputs are automatically set when calling the model,\n",
            " |        `model.fit`, `model.evaluate` or `model.predict`.\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the\n",
            " |      [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint)\n",
            " |      for details on the TensorFlow format.\n",
            " |      \n",
            " |      Args:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If `h5py` is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None, expand_nested=False)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Args:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |          expand_nested: Whether to expand the nested models.\n",
            " |              If not provided, defaults to `False`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
            " |                model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
            " |                multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |                the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Args:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      Note: Since TF 2.6, this method is no longer supported and will raise a\n",
            " |      RuntimeError.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: announces that the method poses a security risk\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      For concrete examples of how to override this method see\n",
            " |      [Customizing what happends in fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Args:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile()`, `add_metric()` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Args:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |        - If the layer is not built, the method will call `build`.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(inputs))\n",
            " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Args:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Args:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  finalize_state(self)\n",
            " |      Finalizes the layers state after updating layer weights.\n",
            " |      \n",
            " |      This function can be subclassed in a layer and will be called after updating\n",
            " |      a layer weights. It can be overridden to finalize any additional layer state\n",
            " |      after a weight update.\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first input node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first output node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function, by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
            " |      and the bias vector. These can be used to set the weights of another\n",
            " |      `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Args:\n",
            " |        weights: a list of NumPy arrays. The number\n",
            " |          of arrays and their shape must match\n",
            " |          number of the dimensions of the weights\n",
            " |          of the layer (i.e. it should match the\n",
            " |          output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If the provided weights list does not match the\n",
            " |          layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Building the model\n",
        "\n",
        "## Input layer\n",
        "input_layer = Input(shape=[28,28], batch_size=32)\n",
        "\n",
        "## Flattening layer\n",
        "flatten_input = Flatten()(input_layer)\n",
        "\n",
        "## BatchNormaliation\n",
        "batch_norm1 = BatchNormalization(name='batchNorm1')(flatten_input)\n",
        "\n",
        "## HiddenLayer1\n",
        "hidden1 = Dense(500, activation=elu, name='hidden1')(batch_norm1)\n",
        "\n",
        "##BatchNorm2\n",
        "batch_norm2 = BatchNormalization(name='batchNorm2')(hidden1)\n",
        "\n",
        "## Hidden 2\n",
        "hidden2 = Dense(100, activation=elu)(batch_norm2)\n",
        "\n",
        "## BatchNorm3\n",
        "batch_norm3 = BatchNormalization(name='batchNorm3')(hidden2)\n",
        "\n",
        "## Hidden 3\n",
        "hidden3 = Dense(50, activation=elu)(batch_norm3)\n",
        "\n",
        "## BatchNorm4\n",
        "batch_norm4 = BatchNormalization(name='batchNorm4')(hidden3)\n",
        "\n",
        "## output layer\n",
        "output_layer = Dense(10, activation=softmax)(batch_norm4)\n",
        "\n",
        "model1 = Model(inputs=[input_layer], outputs=[output_layer])"
      ],
      "metadata": {
        "id": "CkpGjh_0ObEq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()   ### Good that I can account for each of the output params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGiludRXQhZl",
        "outputId": "df2c329f-1e34-46cb-f77e-deb726ae830d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(32, 28, 28)]            0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (32, 784)                 0         \n",
            "                                                                 \n",
            " batchNorm1 (BatchNormalizat  (32, 784)                3136      \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " hidden1 (Dense)             (32, 500)                 392500    \n",
            "                                                                 \n",
            " batchNorm2 (BatchNormalizat  (32, 500)                2000      \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_11 (Dense)            (32, 100)                 50100     \n",
            "                                                                 \n",
            " batchNorm3 (BatchNormalizat  (32, 100)                400       \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_12 (Dense)            (32, 50)                  5050      \n",
            "                                                                 \n",
            " batchNorm4 (BatchNormalizat  (32, 50)                 200       \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_13 (Dense)            (32, 10)                  510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 453,896\n",
            "Trainable params: 451,028\n",
            "Non-trainable params: 2,868\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_main_model = model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mKApx7r-ReDL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert y from list to categorical"
      ],
      "metadata": {
        "id": "ClDDcvgUSl3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train_main)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it9FeHt9S6Ai",
        "outputId": "ef41a664-e511-42f6-d6e3-e3a726eb3295"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_main2 = to_categorical(y_train_main)\n",
        "y_test_main2 = to_categorical(y_test_main)"
      ],
      "metadata": {
        "id": "d1BYUmauS3Jk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_main2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi7-KeHUXftc",
        "outputId": "47a65de9-826f-484e-e37b-85db9f3db9bd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "RkJb8A39aeoh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('/saved_models')"
      ],
      "metadata": {
        "id": "XJTTNUyNahFH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStop = EarlyStopping(patience=5)\n",
        "modelcp = ModelCheckpoint(filepath='/saved_models/FasionMnistMainModel.h5', save_best_only=True)\n"
      ],
      "metadata": {
        "id": "-dLdRJlYZP3v"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(x=X_train_main, y=y_train_main2, epochs=100, callbacks=[earlyStop, modelcp], validation_data=(X_test_main, y_test_main2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMiGFGWAXi2E",
        "outputId": "747ee10a-4828-4264-82c9-b7cfaad2f243"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 20s 12ms/step - loss: 0.3123 - accuracy: 0.8929 - val_loss: 0.2483 - val_accuracy: 0.9144\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 20s 13ms/step - loss: 0.2279 - accuracy: 0.9198 - val_loss: 0.2442 - val_accuracy: 0.9162\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 0.2018 - accuracy: 0.9294 - val_loss: 0.2210 - val_accuracy: 0.9250\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 20s 13ms/step - loss: 0.1811 - accuracy: 0.9350 - val_loss: 0.2136 - val_accuracy: 0.9294\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1655 - accuracy: 0.9404 - val_loss: 0.2048 - val_accuracy: 0.9340\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1514 - accuracy: 0.9449 - val_loss: 0.1958 - val_accuracy: 0.9367\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 19s 13ms/step - loss: 0.1396 - accuracy: 0.9490 - val_loss: 0.2007 - val_accuracy: 0.9362\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1303 - accuracy: 0.9532 - val_loss: 0.2055 - val_accuracy: 0.9320\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1232 - accuracy: 0.9546 - val_loss: 0.1987 - val_accuracy: 0.9419\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1147 - accuracy: 0.9568 - val_loss: 0.2279 - val_accuracy: 0.9364\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1081 - accuracy: 0.9601 - val_loss: 0.2215 - val_accuracy: 0.9362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbfc24d190>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Prediction"
      ],
      "metadata": {
        "id": "38BwEslkpk9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot of train and validation loss"
      ],
      "metadata": {
        "id": "a6W_-hhLpsfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.history.history"
      ],
      "metadata": {
        "id": "50wLB79UpqKt",
        "outputId": "2934b91c-7711-4474-d2ed-887711ab3f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.evaluate(X_test_main, y_test_main2)"
      ],
      "metadata": {
        "id": "9rlJAU9La36Q",
        "outputId": "a0f04e42-4fdd-4a5c-9fec-910d8b50296d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 1s 4ms/step - loss: 0.2215 - accuracy: 0.9362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2214941382408142, 0.9362499713897705]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple prediction\n",
        "plt.imshow(X_train_main[5])"
      ],
      "metadata": {
        "id": "1mkN-FkshhDP",
        "outputId": "43096223-fcd7-403f-ca43-f3a7f49a2ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcbf9f80210>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = np.argmax(model1.predict(X_train_main[:32]), axis=1)"
      ],
      "metadata": {
        "id": "3-5k3RFlidUK"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "id": "Zn9NW2USkeOz",
        "outputId": "a82e8be6-e2a9-43d9-dfa3-85a748aeec6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0, 2, 2, 5, 5, 0, 9, 5, 5, 9, 1, 0, 4, 3, 1, 4, 8, 2,\n",
              "       3, 0, 2, 4, 4, 5, 3, 0, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_main[:32]"
      ],
      "metadata": {
        "id": "Bmh2tsfRl-QJ",
        "outputId": "9dd1a8e9-c48f-417c-f96a-d1fb022950c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0, 2, 2, 5, 5, 0, 9, 5, 5, 9, 1, 0, 4, 3, 1, 4, 8, 4,\n",
              "       3, 0, 2, 4, 4, 5, 3, 0, 8, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train_main[22])"
      ],
      "metadata": {
        "id": "uz7y_Q6JmGPj",
        "outputId": "e831afdd-62b1-4058-ca31-394622c47ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcbfaf5ab90>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the weights and biases of the trained hidden layers\n",
        "\n",
        "These would then be used for training the `transfer-learning` classifier"
      ],
      "metadata": {
        "id": "48LSt8_3m7XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(model1.layers[3].bias)"
      ],
      "metadata": {
        "id": "u-4KG6EBmZRJ",
        "outputId": "b5e64cba-dde8-4ba7-d022-4cb3c4dc4cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcbfae123d0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting to note that the distribution of the `bias` for any partivular hidden layer is normal. This is due to the fact that `BatchNormalization` was used."
      ],
      "metadata": {
        "id": "0h-q53ixooZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating model2\n",
        "\n",
        "This would be using the layers from the model1"
      ],
      "metadata": {
        "id": "eg_3PK1Xm6sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "id": "YI-08qBcpeNA",
        "outputId": "80f8ce56-e02f-4e6c-9a91-fedbac5b7030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(32, 28, 28)]            0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (32, 784)                 0         \n",
            "                                                                 \n",
            " batchNorm1 (BatchNormalizat  (32, 784)                3136      \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " hidden1 (Dense)             (32, 500)                 392500    \n",
            "                                                                 \n",
            " batchNorm2 (BatchNormalizat  (32, 500)                2000      \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_11 (Dense)            (32, 100)                 50100     \n",
            "                                                                 \n",
            " batchNorm3 (BatchNormalizat  (32, 100)                400       \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_12 (Dense)            (32, 50)                  5050      \n",
            "                                                                 \n",
            " batchNorm4 (BatchNormalizat  (32, 50)                 200       \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_13 (Dense)            (32, 10)                  510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 453,896\n",
            "Trainable params: 451,028\n",
            "Non-trainable params: 2,868\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model1.layers)"
      ],
      "metadata": {
        "id": "MeZ6-hL3oHdQ",
        "outputId": "cefe7ed9-184d-4f93-f3d0-83c970f6c4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2ft0aWLypWmc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}